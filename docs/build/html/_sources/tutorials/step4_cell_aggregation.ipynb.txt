{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find Cell Neighbors Using K-Nearest Neighbors (KNN)\n",
    "\n",
    "In this step, we will use the K-Nearest Neighbors (KNN) algorithm to find the nearest neighboring cells for each cell in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DOLPHIN model results from the h5ad file\n",
    "adata = anndata.read_h5ad(\"DOLPHIN_Z.h5ad\")\n",
    "\n",
    "# Define the number of neighbors (default is 10, including the main cell itself)\n",
    "n_neighbor = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_conn_new = kneighbors_graph(adata.obsm['X_z'], 10, mode='connectivity',include_self=True, n_jobs=20).toarray()\n",
    "cell_dist_new = kneighbors_graph(adata.obsm['X_z'], 10, mode='distance', include_self=True,n_jobs=20).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the neighborhood information for 0.701\n",
    "main_name = []\n",
    "combine_name = []\n",
    "for _cell_idx in range(0, adata.obs.shape[0]):\n",
    "    print(\"main_sample\", adata.obs.index[_cell_idx])\n",
    "    for i, _idx  in enumerate(np.nonzero(cell_conn_new[_cell_idx])[0]):\n",
    "        print(adata.obs.index[_idx])\n",
    "        main_name.append(adata.obs.index[_cell_idx])\n",
    "        combine_name.append(adata.obs.index[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"main_name\": main_name, \"neighbor\":combine_name}).to_csv(\"DOLPHIN_aggregation_KNN10.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Cell Aggregation - Adding Junction Reads from Neighboring Cells\n",
    "\n",
    "In this step, we will perform cell aggregation by incorporating confident junction reads from neighboring cells. This process enhances the signal for alternative splicing analysis and helps to resolve potential noise by taking into account the junction read patterns of nearby cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Number of Reads per BAM File for Library Size Normalization\n",
    "\n",
    "```bash\n",
    "find ./02_exon_std -type f -name \"*.bam\" | while read file\n",
    "do  \n",
    "    echo \"$file\"\n",
    "    echo \"$file\" >> get_single_bam_num.txt\n",
    "    samtools flagstat $file >> get_single_bam_num.txt\n",
    "done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the number of reads per bam files\n",
    "with open(\"./get_single_bam_num.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "samples_line = [x.replace(\"\\n\", '').split(\"/\")[-1].split(\".\")[0] for x in lines[0::14]]\n",
    "count_line = lines[1::14]\n",
    "cnt = [int(x.split(\" \")[0]) for x in count_line]\n",
    "pd_cnt = pd.DataFrame({\"sample\":samples_line, \"num_seqs\":cnt})\n",
    "pd_cnt.to_csv(\"single_bam_count.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = \"your_metaData.csv\"\n",
    "pd_gt = pd.read_csv(metadata, sep=\"\\t\")\n",
    "sample_list = list(pd_gt[pd_gt.columns[0]]) \n",
    "pd_aggr = pd.read_csv(\"./DOLPHIN_aggregation_KNN10.csv\")\n",
    "pd_single_size = pd.read_csv(\"./single_bam_count.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## src_path: path to the original bam and sj files\n",
    "src_path = \"./02_exon_std/\"\n",
    "## dist_path: path to the final bam files\n",
    "dist_path = \"./DOLPHIN_aggregation/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in sample_list:\n",
    "    print(target)\n",
    "    target_size = pd_single_size[pd_single_size[\"sample\"] == target].iloc[0][\"num_seqs\"]\n",
    "    _neighbor = list(pd_aggr[pd_aggr[\"main_name\"] == target][\"neighbor\"])\n",
    "    os.makedirs(os.path.join(dist_path, target))\n",
    "    '''\n",
    "    Majority voting: find the frequent junction reads\n",
    "    '''\n",
    "    for _i, _temp_n in enumerate(_neighbor):\n",
    "        _df_junc = pd.read_csv(os.path.join(src_path, _temp_n, _temp_n+\".std.SJ.out.tab\"), sep=\"\\t\",usecols=[0, 1, 2, 7], names=[\"chr\", \"first_base\", \"last_base\",\"multi_map\"+_temp_n])\n",
    "        if _i == 0:\n",
    "            df_merge = _df_junc\n",
    "        else:\n",
    "            df_merge = pd.merge(df_merge, _df_junc, how=\"outer\", left_on=[\"chr\", \"first_base\", \"last_base\"], right_on=[\"chr\", \"first_base\", \"last_base\"])\n",
    "    ## count the occurence of the neighborhood junctions reads, only keep junction reads which is exist in half of the neighbor cells\n",
    "    df_merge[\"nont_na\"] = n_neighbor - df_merge.drop(columns=[\"chr\", \"first_base\", \"last_base\"]).isna().sum(axis=1)\n",
    "    df_keep_junct = df_merge[df_merge[\"nont_na\"] >=5]\n",
    "    ## save to bed file\n",
    "    df_keep_junct[[\"chr\", \"first_base\", \"last_base\"]].to_csv(os.path.join(dist_path, target, \"keep_junction.bed\"), sep=\"\\t\", index=False, header=False)\n",
    "    '''\n",
    "    Bam file batch size normalization\n",
    "    '''\n",
    "    for _n in _neighbor:\n",
    "        _n_seq = pd_single_size[pd_single_size[\"sample\"] == _n].iloc[0][\"num_seqs\"]\n",
    "        shutil.copyfile(os.path.join(src_path, _n, _n+\".std.Aligned.sortedByCoord.out.bam\"), os.path.join(dist_path, target, _n+\".bam\"))\n",
    "        if _n_seq == target_size:\n",
    "            os.rename(os.path.join(dist_path, target, _n+\".bam\"), os.path.join(dist_path, target, _n+\".norm.bam\"))\n",
    "        ##===== Upsampling:\n",
    "        elif _n_seq < target_size: \n",
    "            ## random sample some of the sequcen and then add together with original one\n",
    "            # concate itself n times, where n is the integer part of target_size/ _n_seq\n",
    "            _cat_self_n = int(target_size/ _n_seq)\n",
    "            if _cat_self_n == 1:\n",
    "                _add_seq_perct = (target_size - _n_seq)/_n_seq\n",
    "            else:\n",
    "                _add_seq_perct = (target_size - _n_seq*_cat_self_n)/_n_seq\n",
    "            ## sample the reset seq reads\n",
    "            os.system(f\"samtools view -b -s {_add_seq_perct} {os.path.join(dist_path, target, _n+'.bam')} > {os.path.join(dist_path, target, _n+'.sample.bam')}\")\n",
    "            ## concatenate all \n",
    "            combine_name = \"\"\n",
    "            current_name = os.path.join(dist_path, target, _n+'.bam')\n",
    "            for i in range(_cat_self_n):\n",
    "                if i == 0:\n",
    "                    combine_name = current_name\n",
    "                else:\n",
    "                    combine_name = combine_name + \" \" + current_name\n",
    "            combine_name = combine_name + \" \" + os.path.join(dist_path, target, _n+'.sample.bam')\n",
    "            result_name = os.path.join(dist_path, target, _n+\".norm.bam\")\n",
    "            os.system(f\"samtools merge {result_name} {combine_name}\")\n",
    "            os.remove(os.path.join(dist_path, target, _n+\".sample.bam\"))\n",
    "            os.remove(os.path.join(dist_path, target, _n+\".bam\"))\n",
    "        ##===== Downsampling:\n",
    "        if _n_seq > target_size: \n",
    "            _keep_seq_perct = target_size/_n_seq\n",
    "            os.system(f\"samtools view -b -s {_keep_seq_perct} {os.path.join(dist_path, target, _n+'.bam')} > {os.path.join(dist_path, target, _n+'.norm.bam')}\")\n",
    "            os.remove(os.path.join(dist_path, target, _n+\".bam\"))\n",
    "        '''\n",
    "        Bam file split to junction readsfile\n",
    "        '''\n",
    "        if _n != target:\n",
    "            os.system(f\"samtools view -h {os.path.join(dist_path, target, _n+'.norm.bam')} | awk '$0 ~ /^@/ || $6 ~ /N/' | samtools view -b > {os.path.join(dist_path, target, _n+'.junction.norm.bam')}\")\n",
    "            os.system(f\"samtools index {os.path.join(dist_path, target, _n+'.junction.norm.bam')}\")\n",
    "            '''\n",
    "            Filter to only keep frequent junctions\n",
    "            '''\n",
    "            os.system(f\"samtools view -h -L {os.path.join(dist_path, target, 'keep_junction.bed')} {os.path.join(dist_path, target, _n+'.junction.norm.bam')} > {os.path.join(dist_path, target, _n+'.mj.junction.norm.bam')}\")\n",
    "    '''\n",
    "    Final concate all normalized bam files    \n",
    "    '''     \n",
    "    # Final concate all normalized fastq files\n",
    "    os.system(f\"samtools merge {os.path.join(dist_path, 'final_bam', target+'.final.bam')} {os.path.join(dist_path, target, '*.mj.junction.norm.bam')}  {os.path.join(dist_path, target, target+'.norm.bam')}\")\n",
    "    shutil.rmtree((os.path.join(dist_path, target)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DOLPHIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
